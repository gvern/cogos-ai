<<<<<<< HEAD
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from pathlib import Path

EMBEDDING_DIM = 768
INDEX_PATH = "embeddings/memory.index"
MEMORY_PATH = "ingested/memory.jsonl"
EMBEDDING_MODEL = SentenceTransformer("all-MiniLM-L6-v2")

# Charge FAISS index + documents
def load_memory():
    index = faiss.read_index(INDEX_PATH)
    with open(MEMORY_PATH, "r", encoding="utf-8") as f:
        docs = [json.loads(line) for line in f]
    return index, docs

# Requ√™te m√©moire par similarit√© + synth√®se LLM
def query_memory(query: str, top_k=5) -> str:
    if not Path(INDEX_PATH).exists() or not Path(MEMORY_PATH).exists():
        return "‚ö†Ô∏è M√©moire non disponible. Lancer `python core/ingest.py`."

    index, docs = load_memory()
    query_vec = EMBEDDING_MODEL.encode(query, convert_to_numpy=True).astype("float32").reshape(1, -1)
    scores, indices = index.search(query_vec, top_k)

    context = "\n\n".join([f"- {docs[i]['text'][:500]}" for i in indices[0] if i < len(docs)])

    prompt = f"""Tu es un assistant personnel qui puise dans les souvenirs de ton utilisateur. 
Voici ce que tu as trouv√© en m√©moire concernant la question : "{query}"

{context}

Donne une r√©ponse claire, fid√®le, et personnelle en fran√ßais.
"""

    # Utilise OpenAI (remplacer par Gemini ou local LLM si besoin)
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content.strip()
=======
def query_memory(query: str) -> str:
    # Placeholder for RAG-based semantic retrieval
    return f"üîç I heard your question: '{query}'. Contextual memory search coming soon!"
>>>>>>> 8159adc914993503bf86350ffdcd5cb351b49b99
