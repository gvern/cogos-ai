"""
File System Crawler
==================

Collecteur principal pour scanner r√©cursivement le syst√®me de fichiers
et extraire tous les documents personnels avec m√©tadonn√©es compl√®tes.
"""

import os
import hashlib
import mimetypes
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Generator
import logging
from dataclasses import dataclass

# Configuration logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FileMetadata:
    """M√©tadonn√©es compl√®tes d'un fichier"""
    path: str
    name: str
    extension: str
    size_bytes: int
    created_at: datetime
    modified_at: datetime
    accessed_at: datetime
    mime_type: str
    hash_sha256: str
    category: str
    priority_score: float


class FileSystemCollector:
    """Collecteur syst√®me de fichiers avec priorisation intelligente"""
    
    def __init__(self, base_path: str = "/Users/gustavevernay"):
        self.base_path = Path(base_path)
        self.priority_extensions = {
            'documents': ['.pdf', '.docx', '.txt', '.md', '.rtf', '.pages'],
            'presentations': ['.pptx', '.key', '.odp'],
            'spreadsheets': ['.xlsx', '.csv', '.numbers'],
            'code': ['.py', '.js', '.ts', '.html', '.css', '.json', '.yaml'],
            'images': ['.jpg', '.png', '.gif', '.tiff', '.heic', '.svg'],
            'audio': ['.mp3', '.m4a', '.wav', '.aac', '.flac'],
            'video': ['.mp4', '.mov', '.avi', '.mkv', '.webm']
        }
        
        # Dossiers √† exclure du scan
        self.excluded_dirs = {
            '.git', '.venv', '__pycache__', 'node_modules', '.cache',
            'Library/Caches', 'Library/Logs', '.Trash', '.npm'
        }
        
        # Stats de collecte
        self.stats = {
            'files_scanned': 0,
            'files_processed': 0,
            'total_size_mb': 0,
            'errors': 0
        }
    
    def scan_and_prioritize(self) -> Generator[FileMetadata, None, None]:
        """
        Scanner r√©cursif avec priorisation par importance
        
        Yields:
            FileMetadata: M√©tadonn√©es de chaque fichier trouv√©
        """
        logger.info(f"üîç D√©but du scan r√©cursif depuis {self.base_path}")
        
        for file_path in self._walk_filesystem():
            try:
                metadata = self._extract_file_metadata(file_path)
                if metadata and self._is_relevant_file(metadata):
                    self.stats['files_processed'] += 1
                    self.stats['total_size_mb'] += metadata.size_bytes / (1024 * 1024)
                    yield metadata
                    
                self.stats['files_scanned'] += 1
                
                # Log de progression tous les 1000 fichiers
                if self.stats['files_scanned'] % 1000 == 0:
                    logger.info(f"üìä Progression: {self.stats['files_scanned']} fichiers scann√©s, "
                              f"{self.stats['files_processed']} retenus")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur processing {file_path}: {e}")
                self.stats['errors'] += 1
                continue
    
    def _walk_filesystem(self) -> Generator[Path, None, None]:
        """Parcours r√©cursif du syst√®me de fichiers"""
        for root, dirs, files in os.walk(self.base_path):
            # Exclure les dossiers syst√®me
            dirs[:] = [d for d in dirs if d not in self.excluded_dirs]
            
            for file in files:
                file_path = Path(root) / file
                
                # Ignorer les fichiers syst√®me et cach√©s
                if not file.startswith('.') and file_path.stat().st_size > 0:
                    yield file_path
    
    def _extract_file_metadata(self, file_path: Path) -> Optional[FileMetadata]:
        """Extraction m√©tadonn√©es compl√®tes d'un fichier"""
        try:
            stat = file_path.stat()
            
            # Calcul hash pour d√©tection doublons
            file_hash = self._calculate_file_hash(file_path)
            
            # D√©tection type MIME
            mime_type, _ = mimetypes.guess_type(str(file_path))
            
            # Cat√©gorisation
            category = self._categorize_file(file_path.suffix.lower())
            
            # Score de priorit√© bas√© sur extension, taille, date
            priority_score = self._calculate_priority_score(file_path, stat)
            
            return FileMetadata(
                path=str(file_path),
                name=file_path.name,
                extension=file_path.suffix.lower(),
                size_bytes=stat.st_size,
                created_at=datetime.fromtimestamp(stat.st_ctime),
                modified_at=datetime.fromtimestamp(stat.st_mtime),
                accessed_at=datetime.fromtimestamp(stat.st_atime),
                mime_type=mime_type or 'application/octet-stream',
                hash_sha256=file_hash,
                category=category,
                priority_score=priority_score
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur m√©tadonn√©es {file_path}: {e}")
            return None
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """Calcul SHA256 pour d√©tection doublons"""
        try:
            hasher = hashlib.sha256()
            with open(file_path, 'rb') as f:
                # Lecture par chunks pour gros fichiers
                for chunk in iter(lambda: f.read(8192), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception:
            return ""
    
    def _categorize_file(self, extension: str) -> str:
        """Cat√©gorisation par extension"""
        for category, extensions in self.priority_extensions.items():
            if extension in extensions:
                return category
        return 'other'
    
    def _calculate_priority_score(self, file_path: Path, stat) -> float:
        """
        Calcul score de priorit√© (0.0 √† 1.0)
        Facteurs: extension, taille, r√©cence, localisation
        """
        score = 0.0
        
        # Bonus par cat√©gorie (documents = plus important)
        category_scores = {
            'documents': 0.4,
            'code': 0.3,
            'presentations': 0.25,
            'spreadsheets': 0.2,
            'images': 0.1,
            'audio': 0.05,
            'video': 0.05
        }
        category = self._categorize_file(file_path.suffix.lower())
        score += category_scores.get(category, 0.0)
        
        # Bonus r√©cence (fichiers modifi√©s r√©cemment)
        days_since_modified = (datetime.now() - datetime.fromtimestamp(stat.st_mtime)).days
        if days_since_modified < 30:
            score += 0.3
        elif days_since_modified < 365:
            score += 0.2
        elif days_since_modified < 365 * 3:
            score += 0.1
        
        # Bonus taille (ni trop petit ni trop gros)
        size_mb = stat.st_size / (1024 * 1024)
        if 0.1 < size_mb < 50:  # Taille optimale
            score += 0.2
        elif size_mb > 100:  # Tr√®s gros fichiers moins prioritaires
            score -= 0.1
        
        # Bonus localisation (Desktop, Documents = plus important)
        important_dirs = ['Desktop', 'Documents', 'Projets', 'Projects']
        if any(dir_name in str(file_path) for dir_name in important_dirs):
            score += 0.1
        
        return min(1.0, max(0.0, score))
    
    def _is_relevant_file(self, metadata: FileMetadata) -> bool:
        """Filtrage des fichiers pertinents"""
        # Ignorer les tr√®s petits fichiers
        if metadata.size_bytes < 100:
            return False
        
        # Ignorer les fichiers syst√®me
        if metadata.name.startswith('.') or metadata.name.startswith('~'):
            return False
        
        # Garder seulement les cat√©gories pertinentes
        if metadata.category == 'other':
            return False
        
        return True
    
    def get_scan_statistics(self) -> Dict:
        """Statistiques du scan"""
        return {
            **self.stats,
            'total_size_gb': round(self.stats['total_size_mb'] / 1024, 2),
            'error_rate': self.stats['errors'] / max(1, self.stats['files_scanned']),
            'processing_rate': self.stats['files_processed'] / max(1, self.stats['files_scanned'])
        }


def main():
    """Test du collecteur syst√®me de fichiers"""
    collector = FileSystemCollector()
    
    # Limitation pour test (premiers 100 fichiers)
    count = 0
    for metadata in collector.scan_and_prioritize():
        print(f"üìÑ {metadata.name} ({metadata.category}) - Score: {metadata.priority_score:.2f}")
        count += 1
        if count >= 100:  # Limitation pour test
            break
    
    print(f"\nüìä Statistiques: {collector.get_scan_statistics()}")


if __name__ == "__main__":
    main()
