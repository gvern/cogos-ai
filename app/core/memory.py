import chromadb
try:
    from sentence_transformers import SentenceTransformer
    SentenceTransformer_available = True
except ImportError as e:
    print(f"Warning: Could not import SentenceTransformer: {e}")
    SentenceTransformer_available = False

EMBEDDING_MODEL = None

def get_embedding_model():
    """Lazy initialization of the embedding model."""
    global EMBEDDING_MODEL
    if EMBEDDING_MODEL is None and SentenceTransformer_available:
        try:
            EMBEDDING_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        except Exception as e:
            print(f"Warning: Could not initialize SentenceTransformer: {e}")
            EMBEDDING_MODEL = None
    return EMBEDDING_MODEL
    
from config.secrets import get_api_key
from openai import OpenAI
from pathlib import Path
import chromadb.errors
from datetime import datetime
import uuid
client = OpenAI(api_key=get_api_key())

# ChromaDB client (local persistent store)
CHROMA_DIR = "embeddings/chroma"
chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)

# Nom de la collection
COLLECTION_NAME = "cogos_memory"

def get_collection():
    try:
        return chroma_client.get_collection(name=COLLECTION_NAME)
    except chromadb.errors.CollectionNotFoundError:
        # Si la collection n'existe pas encore, la cr√©er vide
        return chroma_client.create_collection(name=COLLECTION_NAME)

def query_memory(query: str, top_k: int = 5) -> str:
    try:
        embedding_model = get_embedding_model()
        if embedding_model is None:
            return "ü§ñ Syst√®me d'embedding non disponible. V√©rifiez la configuration des d√©pendances."
            
        collection = get_collection()

        # Embedding de la requ√™te
        query_vec = embedding_model.encode(query).tolist()

        # Recherche vectorielle
        results = collection.query(
            query_embeddings=[query_vec],
            n_results=top_k
        )

        docs = results.get("documents", [[]])[0]

        if not docs:
            return "ü§ñ Aucun souvenir trouv√© en m√©moire."

        # Contexte concat√©n√©
        context = "\n\n".join([f"- {doc[:500]}" for doc in docs])

        prompt = f"""Tu es un assistant personnel qui puise dans les souvenirs de ton utilisateur. 
Voici ce que tu as trouv√© en m√©moire concernant la question : "{query}"

{context}

Donne une r√©ponse claire, fid√®le, et personnelle en fran√ßais.
"""

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )

        return response.choices[0].message.content.strip()

    except chromadb.errors.NoIndexException:
        return "‚ö†Ô∏è Index m√©moire manquant. Lance `python core/ingest.py` pour construire la m√©moire."
    except Exception as e:
        return f"‚ö†Ô∏è Erreur m√©moire : {str(e)}"

def add_memory_entry(content: str, tags: list = None, source: str = None) -> bool:
    """
    Ajoute une entr√©e dans la m√©moire vectorielle.
    
    Args:
        content: Le contenu textuel de l'entr√©e
        tags: Liste de tags pour cat√©goriser l'entr√©e
        source: Source de l'information (optionnel)
        
    Returns:
        bool: True si l'ajout a r√©ussi, False sinon
    """
    try:
        embedding_model = get_embedding_model()
        if embedding_model is None:
            print("Warning: EMBEDDING_MODEL is not available. Cannot add entry.")
            return False
            
        collection = get_collection()
        
        # G√©n√©rer un ID unique
        entry_id = str(uuid.uuid4())
        
        # Cr√©er les m√©tadonn√©es
        metadata = {
            "timestamp": datetime.now().isoformat(),
            "source": source or "direct_input"
        }
        
        # Ajouter les tags aux m√©tadonn√©es s'ils existent
        if tags:
            metadata["tags"] = ",".join(tags)
        
        # Encoder le contenu
        embedding = embedding_model.encode(content).tolist()
        
        # Ajouter √† la collection
        collection.add(
            ids=[entry_id],
            embeddings=[embedding],
            documents=[content],
            metadatas=[metadata]
        )
        
        return True
    except Exception as e:
        print(f"Erreur lors de l'ajout √† la m√©moire: {str(e)}")
        return False

def get_recent_entries(limit: int = 10) -> list:
    """
    R√©cup√®re les entr√©es les plus r√©centes de la m√©moire.
    
    Args:
        limit: Nombre maximum d'entr√©es √† r√©cup√©rer
        
    Returns:
        list: Liste des entr√©es r√©centes avec leurs m√©tadonn√©es
    """
    try:
        collection = get_collection()
        
        # R√©cup√©rer toutes les entr√©es (Chroma n'a pas de tri int√©gr√©)
        results = collection.get()
        
        entries = []
        
        # Traiter les r√©sultats
        for i, doc in enumerate(results.get("documents", [])):
            metadata = results.get("metadatas", [])[i]
            entry_id = results.get("ids", [])[i]
            
            # Extraire les tags
            tags = []
            if metadata and "tags" in metadata:
                tags = metadata["tags"].split(",")
            
            entry = {
                "content": doc,
                "timestamp": metadata.get("timestamp", ""),
                "tags": tags,
                "source": metadata.get("source", ""),
                "embedding_id": entry_id
            }
            
            entries.append(entry)
        
        # Trier par timestamp (du plus r√©cent au plus ancien)
        entries.sort(key=lambda x: x["timestamp"], reverse=True)
        
        # Limiter le nombre d'entr√©es
        return entries[:limit]
    except Exception as e:
        print(f"Erreur lors de la r√©cup√©ration des entr√©es r√©centes: {str(e)}")
        return []
